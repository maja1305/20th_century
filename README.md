
# 20th Century Text Mining Project 
This project explores Wikipedia text data to identify historical patterns and country mentions during the 20th century.  
It was developed as part of my Data Analytics course to apply real-world web scraping, text cleaning, and text mining techniques using Python.
## Project Summary
The goal of this project is to practice Natural Language Processing (NLP) and text analysis by:
- Scraping textual data from Wikipedia  
- Cleaning and preprocessing the text  
- Extracting named entities (countries)  
- Analyzing frequency and distribution of country mentions  
- Interpreting results in a historical context  
## Project Structure  
## Folders  

- **Data/** – contains raw and cleaned data files 
- **Scripts/** – contains all Jupyter notebooks and Python scripts  
- **Imgs/** – contains visaualizations and images 
- **Docs/** – optional folder for documentation
- **Requirements/** – lists Python dependencies for reproducibility  


| File | Description |
| **1.4_20th_century_scrape_text.ipynb** | Web scraping the Wikipedia page and saving the text data |
| **1.4b_20th_century_scrape_countries.ipynb** | Extracting and counting country mentions |
| **1.5_20th_century_text_cleaning.ipynb** | Further text preprocessing and cleaning |
| **1.6_20th_century_text_mining.ipynb** | Entity recognition, frequency analysis, and visualization |
| **1.7_20th_century_network_visualization.ipynb** | Creating and visualizing a network country relationships |
